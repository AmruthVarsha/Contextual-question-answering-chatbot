{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install fuzzywuzzy[speedup]  # or rapidfuzz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA96xNsgTRRK",
        "outputId": "145b8eb4-3f7a-4420-ae6c-d6048ed43ec7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy[speedup] in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-levenshtein>=0.12 in /usr/local/lib/python3.10/dist-packages (from fuzzywuzzy[speedup]) (0.25.1)\n",
            "Requirement already satisfied: Levenshtein==0.25.1 in /usr/local/lib/python3.10/dist-packages (from python-levenshtein>=0.12->fuzzywuzzy[speedup]) (0.25.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.25.1->python-levenshtein>=0.12->fuzzywuzzy[speedup]) (3.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n11uGdgWSOMd",
        "outputId": "6ba8cd43-dcf5-45a1-8a85-8e686bb45323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please ask your question: what form of poetry was developed in yuan\n",
            "Closest Question Found: What form of poetry was developed in the Yuan?\n",
            "Context: In the China of the Yuan, or Mongol era, various important developments in the arts occurred or continued in their development, including the areas of painting, mathematics, calligraphy, poetry, and theater, with many great artists and writers being famous today. Due to the coming together of painting, poetry, and calligraphy at this time many of the artists practicing these different pursuits were the same individuals, though perhaps more famed for one area of their achievements than others. Often in terms of the further development of landscape painting as well as the classical joining together of the arts of painting, poetry, and calligraphy, the Song dynasty and the Yuan dynasty are linked together. In the area of Chinese painting during the Yuan dynasty there were many famous painters. In the area of calligraphy many of the great calligraphers were from the Yuan dynasty era. In Yuan poetry, the main development was the qu, which was used among other poetic forms by most of the famous Yuan poets. Many of the poets were also involved in the major developments in the theater during this time, and the other way around, with people important in the theater becoming famous through the development of the sanqu type of qu. One of the key factors in the mix of the zaju variety show was the incorporation of poetry both classical and of the newer qu form. One of the important cultural developments during the Yuan era was the consolidation of poetry, painting, and calligraphy into a unified piece of the type that tends to come to mind when people think of classical Chinese art. Another important aspect of Yuan times is the increasing incorporation of the then current, vernacular Chinese into both the qu form of poetry and the zaju variety show. Another important consideration regarding Yuan dynasty arts and culture is that so much of it has survived in China, relatively to works from the Tang dynasty and Song dynasty, which have often been better preserved in places such as the Shōsōin, in Japan.\n",
            "Predicted Answer: qu\n"
          ]
        }
      ],
      "source": [
        "from fuzzywuzzy import process\n",
        "import json\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "with open('/content/dev-v1.1.json', 'r') as f:\n",
        "    squad_data = json.load(f)\n",
        "\n",
        "# Preprocess the dataset to map questions to their corresponding context\n",
        "question_context_map = {}\n",
        "\n",
        "for article in squad_data['data']:\n",
        "    for paragraph in article['paragraphs']:\n",
        "        context = paragraph['context']\n",
        "        for qa in paragraph['qas']:\n",
        "            question = qa['question']\n",
        "            question_context_map[question] = context\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def get_closest_question(input_question):\n",
        "    \"\"\"\n",
        "    Use fuzzy matching to find the closest question in the SQuAD dataset\n",
        "    to the one provided by the user.\n",
        "    \"\"\"\n",
        "    questions = list(question_context_map.keys())\n",
        "    closest_question, _ = process.extractOne(input_question, questions)\n",
        "    return closest_question\n",
        "\n",
        "def answer_question_auto(question):\n",
        "    \"\"\"\n",
        "    Takes a question as input, retrieves the closest matching context\n",
        "    from the dataset, and uses the fine-tuned T5 model to generate an answer.\n",
        "    \"\"\"\n",
        "    # Get the closest question based on the input question\n",
        "    closest_question = get_closest_question(question)\n",
        "    context = question_context_map[closest_question]\n",
        "\n",
        "    print(f\"Closest Question Found: {closest_question}\")\n",
        "    print(f\"Context: {context}\")\n",
        "\n",
        "    # Combine question and context for the model input\n",
        "    input_text = f\"question: {question} context: {context}\"\n",
        "\n",
        "    # Encode input text using the tokenizer\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output (answer)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "    # Decode the generated output to readable text\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Now you can input any question\n",
        "user_question = input(\"Please ask your question: \")\n",
        "predicted_answer = answer_question_auto(user_question)\n",
        "print(f\"Predicted Answer: {predicted_answer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "with open('/content/dev-v1.1.json', 'r') as f:\n",
        "    squad_data = json.load(f)\n",
        "\n",
        "# Preprocess the dataset to map questions to their corresponding context\n",
        "question_context_map = {}\n",
        "question_answer_map = {}\n",
        "\n",
        "for article in squad_data['data']:\n",
        "    for paragraph in article['paragraphs']:\n",
        "        context = paragraph['context']\n",
        "        for qa in paragraph['qas']:\n",
        "            question = qa['question']\n",
        "            answer = qa['answers'][0]['text'] if qa['answers'] else ''\n",
        "            question_context_map[question] = context\n",
        "            question_answer_map[question] = answer\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def get_closest_question(input_question):\n",
        "    \"\"\"\n",
        "    Use fuzzy matching to find the closest question in the SQuAD dataset\n",
        "    to the one provided by the user.\n",
        "    \"\"\"\n",
        "    questions = list(question_context_map.keys())\n",
        "    closest_question, _ = process.extractOne(input_question, questions)\n",
        "    return closest_question\n",
        "\n",
        "def answer_question_auto(question):\n",
        "    \"\"\"\n",
        "    Takes a question as input, retrieves the closest matching context\n",
        "    from the dataset, and uses the fine-tuned T5 model to generate an answer.\n",
        "    \"\"\"\n",
        "    # Get the closest question based on the input question\n",
        "    closest_question = get_closest_question(question)\n",
        "    context = question_context_map[closest_question]\n",
        "\n",
        "    print(f\"Closest Question Found: {closest_question}\")\n",
        "    print(f\"Context: {context}\")\n",
        "\n",
        "    # Combine question and context for the model input\n",
        "    input_text = f\"question: {question} context: {context}\"\n",
        "\n",
        "    # Encode input text using the tokenizer\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output (answer)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids, max_length=50, num_beams=3, early_stopping=True)\n",
        "\n",
        "    # Decode the generated output to readable text\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def sample_random_subset(data, num_samples=50000):\n",
        "    \"\"\"\n",
        "    Randomly sample a subset of the dataset.\n",
        "    \"\"\"\n",
        "    questions = list(data.keys())\n",
        "    sampled_questions = random.sample(questions, num_samples)\n",
        "    return {q: data[q] for q in sampled_questions}\n",
        "\n",
        "def answer_questions_batch(questions, contexts):\n",
        "    \"\"\"\n",
        "    Generate answers for a batch of questions and contexts.\n",
        "    \"\"\"\n",
        "    input_texts = [f\"question: {q} context: {c}\" for q, c in zip(questions, contexts)]\n",
        "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=30, num_beams=3, early_stopping=True)\n",
        "\n",
        "    answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return answers\n",
        "\n",
        "def evaluate_model_on_subset(subset_question_context_map, batch_size=8):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a subset of the dataset.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    questions = list(subset_question_context_map.keys())\n",
        "    contexts = [subset_question_context_map[q] for q in questions]\n",
        "    ground_truths = [question_answer_map.get(q, '') for q in questions]\n",
        "\n",
        "    # Process in batches\n",
        "    for start_idx in range(0, len(questions), batch_size):\n",
        "        end_idx = min(start_idx + batch_size, len(questions))\n",
        "        batch_questions = questions[start_idx:end_idx]\n",
        "        batch_contexts = contexts[start_idx:end_idx]\n",
        "        batch_ground_truths = ground_truths[start_idx:end_idx]\n",
        "\n",
        "        batch_predictions = answer_questions_batch(batch_questions, batch_contexts)\n",
        "        predictions.extend(batch_predictions)\n",
        "        references.extend(batch_ground_truths)\n",
        "\n",
        "    # Compute metrics\n",
        "    em_score = np.mean([p.lower() == r.lower() for p, r in zip(predictions, references)])\n",
        "    f1_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_tokens = set(pred.lower().split())\n",
        "        ref_tokens = set(ref.lower().split())\n",
        "        precision = len(pred_tokens & ref_tokens) / len(pred_tokens) if pred_tokens else 0\n",
        "        recall = len(pred_tokens & ref_tokens) / len(ref_tokens) if ref_tokens else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    avg_f1_score = np.mean(f1_scores)\n",
        "\n",
        "    print(f\"Exact Match Score: {em_score * 100:.2f}%\")\n",
        "    print(f\"Average F1 Score: {avg_f1_score:.2f}\")\n",
        "\n",
        "# Use the subset for evaluation\n",
        "subset_question_context_map = sample_random_subset(question_context_map, num_samples=100)\n",
        "evaluate_model_on_subset(subset_question_context_map, batch_size=8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaFzUstkYkjB",
        "outputId": "855717c6-63f5-4d41-a5e2-efd6ab48909e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match Score: 59.00%\n",
            "Average F1 Score: 0.77\n"
          ]
        }
      ]
    }
  ]
}